[
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster SPRM output of CODEX data and visualize with Vitessce\n",
    "This template shows how to cluster CODEX data and load a new clustering into Vitessce. This template requires prior cell segmentation, and takes in the locations of cell (centers) and protein expression per cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas requests wheel matplotlib matplotlib-inline scikit-learn vitessce==3.2.6 starlette uvicorn widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import anndata as ad\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from vitessce import VitessceChainableConfig, AnnDataWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linked datasets\n",
    "The following datasets were symlinked to the workspace when this template was added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linked datasets\n",
    "uuids = {{ uuids | safe }}\n",
    "\n",
    "# accepted datatypes \n",
    "accepted_datatypes = ['CODEX [Cytokit + SPRM]']\n",
    "\n",
    "# required filetypes\n",
    "required_filetypes = ['sprm_outputs/reg001_expr.ome.tiff-cell_channel_total.csv', 'sprm_outputs/reg001_expr.ome.tiff-cell_channel_mean.csv', 'sprm_outputs/reg001_expr.ome.tiff-cell_centers.csv']\n",
    "\n",
    "# search_api\n",
    "search_api = 'https://search.api.hubmapconsortium.org/v3/portal/search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following checks if the datasets are compatible with this template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This template is created for particular datatypes only.\n",
    "# This functions checks for each uuids above whether they have the correct datatypes.\n",
    "\n",
    "def check_template_compatibility(uuids, accepted_datatypes=None, required_filetypes=None, search_api = 'https://search.api.hubmapconsortium.org/v3/portal/search'): \n",
    "    '''\n",
    "    For a set of HuBMAP UUIDs, check if valid, and return valid UUIDs.\n",
    "    Checks if UUIDs are present in the search API. \n",
    "    If accepted_datatypes is defined, checks if UUIDs are of any of the datatypes in accepted_datatypes.\n",
    "    If required_filetypes is defined, checks if UUIDs have all of the required filetypes in required_filetypes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uuids : array of string\n",
    "        HuBMAP UUIDs to be checked\n",
    "    accepted_datatypes: array of string, optional\n",
    "        accepted datatypes for template\n",
    "    required_filetypes: array of string, optional\n",
    "        required datatypes for template\n",
    "    search_api: string, optional\n",
    "        URL of search API\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array of string\n",
    "        valid UUIDs\n",
    "    '''\n",
    "    hits = json.loads(\n",
    "        requests.post(\n",
    "            search_api,\n",
    "            json={\n",
    "                'size': 10000,\n",
    "                'query': {'ids': {'values': uuids}},\n",
    "                '_source': ['files', 'assay_display_name']\n",
    "            }, \n",
    "        ).text\n",
    "    )['hits']['hits']\n",
    "\n",
    "    # create mapping for uuid to file_types and assay_display_name\n",
    "    uuid_to_files = {}\n",
    "    uuid_to_datatypes = {}\n",
    "    for hit in hits:\n",
    "        file_paths = [file['rel_path'] for file in hit['_source']['files']]\n",
    "        uuid_to_files[hit['_id']] = file_paths\n",
    "\n",
    "        hit_data_type = hit['_source']['assay_display_name']\n",
    "        uuid_to_datatypes[hit['_id']] = hit_data_type\n",
    "    \n",
    "    # save uuids without warnings\n",
    "    accepted_uuids = uuids.copy()\n",
    "\n",
    "    # remove unvalid uuids\n",
    "    for uuid in uuids: \n",
    "        # check if all uuids are found in the search api\n",
    "        if uuid not in uuid_to_files.keys(): \n",
    "            warnings.warn('Dataset with UUID \"' + uuid + '\" not found in Search API')\n",
    "            accepted_uuids.remove(uuid)\n",
    "            continue\n",
    "\n",
    "        if required_filetypes is not None: \n",
    "            # check if file_types for each uuid are in required_filetypes\n",
    "            file_types = uuid_to_files[uuid]\n",
    "            for required_file_type in required_filetypes:\n",
    "                if required_file_type not in file_types:\n",
    "                    warnings.warn('Dataset with UUID \"' + uuid + '\" does not have required file type: ' + required_file_type)\n",
    "                    if uuid in accepted_uuids:\n",
    "                        accepted_uuids.remove(uuid)\n",
    "\n",
    "        if accepted_datatypes is not None: \n",
    "            # check if assay_display_name for each uuid are in accepted_datatypes\n",
    "            assay_display_name = uuid_to_datatypes[uuid]\n",
    "            for data_type in assay_display_name:\n",
    "                if data_type not in accepted_datatypes: \n",
    "                    warnings.warn('Dataset with UUID \"' + uuid + '\" has unaccepted data type: ' + data_type)\n",
    "                    if uuid in accepted_uuids:\n",
    "                        accepted_uuids.remove(uuid)\n",
    "                    continue\n",
    "    \n",
    "    return accepted_uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = check_template_compatibility(uuids, accepted_datatypes=accepted_datatypes, required_filetypes=required_filetypes, search_api=search_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval\n",
    "\n",
    "This template will work with the total and mean expression as determined by the SPRM pipeline, and will use the determined cell centers. If the datasets are symlinked with the workspace, these files are already available locally. Otherwise, below shows how to retrieve these specific files through the assets API. As these are relatively small csv files, this is feasible. However, for many files or larger files, it is faster to symlink the datasets in a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_sprm_file(uuid, file_name, root_folder='.'): \n",
    "    '''\n",
    "    For a given UUID and SPRM output file name, retrieve this file and save it locally.\n",
    "\n",
    "    Parameters: \n",
    "        uuid (string): UUID of dataset\n",
    "        file_name (string): desired file\n",
    "    '''\n",
    "    if not os.path.exists('datasets/' + uuid + '/sprm_outputs'):\n",
    "        # unlike os.mkdir, os.makedirs creates directories recursively\n",
    "        os.makedirs('datasets/' + uuid + '/sprm_outputs', exist_ok = True)\n",
    "\n",
    "    sprm_url = 'https://assets.hubmapconsortium.org/' + uuid + '/sprm_outputs/' + file_name\n",
    "    res = requests.get(sprm_url)\n",
    "    if res.status_code != 200:\n",
    "        if res.status_code == 404: \n",
    "            raise FileNotFoundError('For uuid:', uuid, 'and file', file_name, 'generated URL does not exist.',\n",
    "                                    'Did you check the file name and whether this is an SPRM dataset? Generated url:', \n",
    "                                    sprm_url, \n",
    "                                    'File should be visible in', 'https://portal.hubmapconsortium.org/browse/dataset/' + uuid)\n",
    "        else: \n",
    "            raise ConnectionError('Error code:', res.status_code, \n",
    "                                  'For uuid:', uuid, 'and file', file_name, '. Did you check the UUID?')\n",
    "    else:\n",
    "        with open(root_folder + '/datasets/' + uuid + '/sprm_outputs/' + file_name, 'wb') as f: \n",
    "            f.write(res.content)\n",
    "\n",
    "\n",
    "def retrieve_sprm_files(uuids, file_names=['reg001_expr.ome.tiff-cell_channel_total.csv', 'reg001_expr.ome.tiff-cell_channel_mean.csv', 'reg001_expr.ome.tiff-cell_centers.csv'], root_folder='.'):\n",
    "    '''\n",
    "    For a given list of UUIDS, retrieve files for sprm analysis and save these locally in datasets folder.\n",
    "\n",
    "    Parameters: \n",
    "        uuids (list): list of uuids of interest\n",
    "        file_name (list): desired files. Default: ['reg001_expr.ome.tiff-cell_channel_total.csv', 'reg001_expr.ome.tiff-cell_channel_mean.csv', 'reg001_expr.ome.tiff-cell_centers.csv']\n",
    "    '''\n",
    "    for uuid in uuids: \n",
    "        for file in file_names: \n",
    "            if not (os.path.exists(root_folder + '/datasets/' + uuid + '/sprm_outputs/' + file)): \n",
    "                retrieve_sprm_file(uuid, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '.'\n",
    "\n",
    "retrieve_sprm_files(uuids, root_folder=root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "This shows how to apply k-means clustering to a particular dataframe, with variable amount of clusters and optional scalers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_pipe(df, n_clusters=10, scaler='z'):\n",
    "    '''\n",
    "    Pipeline for applying preprocessing and k-means clustering to dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        dataframe with protein expressions per cell\n",
    "    n_clusters : int, optional\n",
    "        number of clusters for k-means clustering. Default: 10\n",
    "    scaler : str, optional\n",
    "        Scaler used for preprocessing. One of 'z', 'minmax' or None. Default: 'z'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame\n",
    "        Result of k-means clustering. Dataframe with 2 columns (ID = cell ID, cluster = cluster label)\n",
    "    '''\n",
    "    if scaler == 'z':\n",
    "        preprocessor = Pipeline(\n",
    "            [\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"pca\", PCA(n_components=2, random_state=42)),\n",
    "            ]\n",
    "        )\n",
    "    elif scaler == 'minmax': \n",
    "        preprocessor = Pipeline(\n",
    "            [\n",
    "                (\"scaler\", MinMaxScaler()),\n",
    "                (\"pca\", PCA(n_components=2, random_state=42)),\n",
    "            ]\n",
    "            )\n",
    "    elif scaler == None:\n",
    "        preprocessor = Pipeline(\n",
    "            [\n",
    "                (\"pca\", PCA(n_components=2, random_state=42)),\n",
    "            ]\n",
    "            )\n",
    "        \n",
    "    else:\n",
    "        warnings.warn(scaler + \" is not a valid scaling option. Defaulting to z-scaler.\")\n",
    "        preprocessor = Pipeline(\n",
    "            [\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"pca\", PCA(n_components=2, random_state=42)),\n",
    "            ]\n",
    "            )\n",
    "    \n",
    "    clusterer = Pipeline(\n",
    "        [\n",
    "         (\n",
    "           \"kmeans\",\n",
    "           KMeans(\n",
    "               n_clusters=n_clusters,\n",
    "               init=\"k-means++\",\n",
    "               n_init=50,\n",
    "               max_iter=500,\n",
    "               random_state=42,\n",
    "           ),\n",
    "       ),\n",
    "        ]\n",
    "    )\n",
    "    pipe = Pipeline(\n",
    "     [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"clusterer\", clusterer)\n",
    "        ]\n",
    "    )\n",
    "    pipe.fit(df)\n",
    "\n",
    "    kmdf = pd.DataFrame()\n",
    "\n",
    "    kmdf['ID'] = df['ID']\n",
    "    kmdf[\"cluster\"] = pipe[\"clusterer\"][\"kmeans\"].labels_\n",
    "    \n",
    "    return kmdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to subset our proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciton for getting all proteins\n",
    "def get_all_proteins(df):\n",
    "    '''\n",
    "    Return a list of all proteins in dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        dataframe with protein expressions per cell\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of string\n",
    "        list with names of proteins\n",
    "    '''\n",
    "    return df.columns\n",
    "\n",
    "#function for creating a subset of a dataframe using a list of columns\n",
    "def protein_subset(df, protein_list, keep=True):\n",
    "    '''\n",
    "    Subset dataframe for certain proteins. \n",
    "    If keep=True, keep the proteins in protein_list. \n",
    "    Otherwise, keep all proteins not in protein_list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        dataframe with protein expressions per cell\n",
    "    protein_list : list of string\n",
    "        list with proteins to keep or discard\n",
    "    keep : Boolean, optional\n",
    "        whether to keep (True) or discard (False) proteins in protein_list. Default: True\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas Dataframe \n",
    "        subsetted dataframe\n",
    "    '''\n",
    "    if keep: \n",
    "        df_sub = df[list]\n",
    "        return df_sub\n",
    "    else: \n",
    "        df_sub = df.drop(df.columns[list],axis=1)\n",
    "        return df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to visualize with Vitessce later, we need to save the created clustering as a separate file. We will split the clustering into three parts: 1) reading in the data, 2) preprocess and cluster, 3) write to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dataframes\n",
    "def read_dfs_expression_location(uuid, df_expression_file='sprm_outputs/reg001_expr.ome.tiff-cell_channel_total.csv', df_location_file='sprm_outputs/reg001_expr.ome.tiff-cell_centers.csv', root_folder='.'):\n",
    "    '''\n",
    "    Read in expression and location dataframes. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uuid : string\n",
    "        UUID of dataset\n",
    "    df_expression_file : string, optional\n",
    "        relative location of expression csv file.\n",
    "        Default: 'sprm_outputs/reg001_expr.ome.tiff-cell_channel_total.csv'\n",
    "    df_location_file : string\n",
    "        relative location of location csv file.\n",
    "        Default: 'sprm_outputs/reg001_expr.ome.tiff-cell_centers.csv'\n",
    "    root_folder : string, optional\n",
    "        top folder of workspace, or parent folder of datasets. Default: '.'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame \n",
    "        dataframe with expression\n",
    "    Pandas DataFrame\n",
    "        dataframe with location\n",
    "    '''\n",
    "    df_expression = pd.read_csv(root_folder + '/datasets/' + uuid + '/' + df_expression_file)\n",
    "    df_location = pd.read_csv(root_folder + '/datasets/' + uuid + '/' + df_location_file)\n",
    "    return [df_expression, df_location]\n",
    "\n",
    "# preprocess and cluster\n",
    "def preprocess_and_cluster(df_expression, df_location, n_clusters=10, scaler='z', subset=False, subset_list=[], keep=True):\n",
    "    '''\n",
    "    Pre-process and k-means cluster dataset in df_expression. Also updates df_expression and df_location to be consistent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uuid : string\n",
    "        UUID of dataset\n",
    "    df_expression : Pandas DataFrame\n",
    "        dataframe with expression.\n",
    "    df_location : Pandas DataFrame\n",
    "        dataframe with cell locations.\n",
    "    n_clusters : int, optional\n",
    "        number of clusters for k-means clustering. Default: 10\n",
    "    scaler : str, optional\n",
    "        Scaler used for preprocessing. One of 'z', 'minmax' or None. Default: 'z'\n",
    "    subset : Boolean, optional\n",
    "        if true, subset dataframe with proteins in subset_list. Default: False\n",
    "    subset_list : list of string\n",
    "        list with proteins to keep. Only used when subset=True. Default: []\n",
    "    keep : Boolean, optional\n",
    "        whether to keep (True) or discard (False) proteins in protein_list. \n",
    "        Only used when subset=True. Default: True\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame \n",
    "        dataframe with expression\n",
    "    Pandas DataFrame\n",
    "        dataframe with location\n",
    "    Pandas DataFrame\n",
    "        dataframe with clusters\n",
    "    '''\n",
    "    cellID = list(df_expression['ID'])\n",
    "    df_location = df_location.loc[df_location['ID'].isin(cellID)]\n",
    "    if subset: \n",
    "        df_expression_subset = protein_subset(df_expression, subset_list)\n",
    "        df_clusters = kmeans_pipe(df_expression_subset, n_clusters=n_clusters, scaler=scaler)\n",
    "    else: \n",
    "        df_clusters = kmeans_pipe(df_expression, n_clusters=n_clusters, scaler=scaler)\n",
    "    \n",
    "    return([df_expression, df_location, df_clusters])\n",
    "\n",
    "# write to files\n",
    "def save_dfs(uuid, df_expression, df_location, df_clusters, root_folder='.'):\n",
    "    '''\n",
    "    Save dataframes locally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uuid : string\n",
    "        UUID of dataset\n",
    "    df_expression : Pandas DataFrame\n",
    "        dataframe with expression.\n",
    "    df_location : Pandas DataFrame\n",
    "        dataframe with cell locations.\n",
    "    root_folder : string, optional\n",
    "        top folder of workspace, or parent folder of datasets. Default: '.'\n",
    "    '''\n",
    "    os.makedirs(root_folder + '/output/' + uuid + '/kmeans_output', exist_ok=True)\n",
    "    df_expression.to_csv(root_folder + '/output/' + uuid + '/kmeans_output/cell_channel_total.csv', index=False)\n",
    "    df_location.to_csv(root_folder + '/output/' + uuid + '/kmeans_output/cell_centers.csv', index=False)\n",
    "    df_clusters.to_csv(root_folder + '/output/' + uuid + '/kmeans_output/cell_cluster.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine reading, clustering, and writing\n",
    "def get_clusters(uuid, \n",
    "                 df_expression_file='sprm_outputs/reg001_expr.ome.tiff-cell_channel_total.csv', \n",
    "                 df_location_file='sprm_outputs/reg001_expr.ome.tiff-cell_centers.csv', \n",
    "                 root_folder='.',\n",
    "                 n_clusters=10, scaler='z', subset=False, subset_list=[], keep=True): \n",
    "    '''\n",
    "    Pre-process and k-means cluster dataset identified with UUID, using total protein levels.\n",
    "    Saves total protein, cell centers and clusters to separate files in kmeans_output folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uuid : string\n",
    "        UUID of dataset\n",
    "    df_expression_file : string, optional\n",
    "        relative location of expression csv file.\n",
    "        Default: 'sprm_outputs/reg001_expr.ome.tiff-cell_channel_total.csv'\n",
    "    df_location_file : string\n",
    "        relative location of location csv file.\n",
    "        Default: 'sprm_outputs/reg001_expr.ome.tiff-cell_centers.csv'\n",
    "    root_folder : string, optional\n",
    "        top folder of workspace, or parent folder of datasets. Default: '.'\n",
    "    n_clusters : int, optional\n",
    "        number of clusters for k-means clustering. Default: 10\n",
    "    scaler : str, optional\n",
    "        Scaler used for preprocessing. One of 'z', 'minmax' or None. Default: 'z'\n",
    "    subset : Boolean, optional\n",
    "        if true, subset dataframe with proteins in subset_list. Default: False\n",
    "    subset_list : list of string\n",
    "        list with proteins to keep. Only used when subset=True. Default: []\n",
    "    keep : Boolean, optional\n",
    "        whether to keep (True) or discard (False) proteins in protein_list. \n",
    "        Only used when subset=True. Default: True\n",
    "    '''\n",
    "    # load in dataframes\n",
    "    df_expression, df_location = read_dfs_expression_location(uuid, df_expression_file=df_expression_file, \n",
    "                                                              df_location_file=df_location_file, root_folder=root_folder)\n",
    "    df_expression, df_location, df_clusters = preprocess_and_cluster(df_expression, df_location, n_clusters=n_clusters, \n",
    "                                                                     scaler=scaler, subset=subset, subset_list=subset_list, keep=keep)\n",
    "    save_dfs(uuid, df_expression, df_location, df_clusters, root_folder=root_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uuid in uuids: \n",
    "    get_clusters(uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize clustering\n",
    "We can now visualize our new clustering. We will first make a static visualization with matplotlib, and then load the new clustering in Vitessce for an interactive visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib \n",
    "def show_kmeans_scatter(uuid, root_folder='.'):\n",
    "    '''\n",
    "    Create matplotlib scatterplot of cells, colored by cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uuid : str\n",
    "        uuid of HuBMAP dataset to visualize.\n",
    "    '''\n",
    "    df_location = pd.read_csv(root_folder + '/output/' + uuid + '/kmeans_output/cell_centers.csv')\n",
    "    df_clusters = pd.read_csv(root_folder + '/output/' + uuid + '/kmeans_output/cell_cluster.csv')\n",
    "\n",
    "    df_location['x'] = df_location['x'].map(lambda x: -x)\n",
    "    col = df_clusters['cluster']\n",
    "    plt.scatter('y','x',c=col,s=.05,data=df_location,cmap='tab10')\n",
    "    plt.rcParams['figure.figsize'] = [8,8]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_kmeans_scatter(uuids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vitessce view\n",
    "We can load our data into vitessce for an interactive view.\n",
    "\n",
    "In the HuBMAP workspaces environment, to load local files, we will first create an AnnDataWrapper for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the anndata object and writing it to a local zarr store\n",
    "\n",
    "uuid = uuids[0]\n",
    "\n",
    "with open(root_folder + '/output/' + uuid + '/kmeans_output/cell_channel_total.csv') as df_expression:\n",
    "    adata = ad.read_csv(df_expression)\n",
    "\n",
    "df_expression = pd.read_csv(root_folder + '/output/' + uuid + '/kmeans_output/cell_channel_total.csv')\n",
    "df_location = pd.read_csv(root_folder + '/output/' + uuid + '/kmeans_output/cell_centers.csv')\n",
    "df_clusters = pd.read_csv(root_folder + '/output/' + uuid + '/kmeans_output/cell_cluster.csv')\n",
    "\n",
    "# df_expression\n",
    "df_expression.drop('ID', axis=1, inplace=True)\n",
    "df_location.drop('ID', axis=1, inplace=True)\n",
    "df_clusters.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "adata = ad.AnnData(X=df_expression,obs=df_clusters,var=pd.DataFrame(columns=df_expression.columns).T)\n",
    "adata.obsm['location'] = np.array(df_location[['x', 'y']])\n",
    "\n",
    "adata.write_zarr(root_folder + '/output/' + uuid + '/anndata.zarr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an AnnDataWrapper\n",
    "\n",
    "w = AnnDataWrapper(\n",
    "    adata_store=root_folder + '/output/' + uuid + '/anndata.zarr',\n",
    "    obs_feature_matrix_path=\"X\",\n",
    "    obs_locations_path=\"obsm/location\",\n",
    "    obs_set_paths=[\"obs/cluster\"],\n",
    "    obs_set_names=['K-means Z-scaling k=10'],\n",
    "    coordination_values={\"featureType\": \"antigen\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Vitessce config\n",
    "\n",
    "conf = VitessceChainableConfig(\n",
    "    name=\"Clustering CODEX data\", description=\"\", schema_version=\"1.0.16\"\n",
    ").add_dataset(\n",
    "    name='Protein dataset', \n",
    "    uid='A',\n",
    "    objs = [w]\n",
    ").set_coordination_value(\n",
    "    c_type=\"spatialSegmentationLayer\",\n",
    "    c_scope=\"A\",\n",
    "    c_value={\n",
    "      \"opacity\": 1,\n",
    "      \"radius\": 20,\n",
    "      \"visible\": True,\n",
    "      \"stroked\": False\n",
    "    }\n",
    ").set_coordination_value(\n",
    "    c_type=\"featureType\",\n",
    "    c_scope=\"A\",\n",
    "    c_value=\"antigen\"\n",
    ").add_view(\n",
    "     dataset_uid=\"A\", component=\"spatial\", x=0, y=0, w=4, h=8,\n",
    "     coordination_scopes={\"spatialSegmentationLayer\": \"A\", \"featureType\": \"A\" }\n",
    ").add_view(\n",
    "    dataset_uid=\"A\", component=\"obsSetSizes\", x=4, y=0, w=4, h=8\n",
    ").add_view(\n",
    "    dataset_uid=\"A\", component=\"obsSets\", x=8, y=0, w=2, h=8\n",
    ").add_view(\n",
    "    dataset_uid=\"A\", component=\"heatmap\", x=0, y=8, w=6, h=6,\n",
    "    coordination_scopes={ \"featureType\": \"A\" },\n",
    "    props={\"transpose\": True},\n",
    ").add_view(\n",
    "    dataset_uid=\"A\", component=\"featureList\", x=6, y=8, w=4, h=6,\n",
    "    coordination_scopes={ \"featureType\": \"A\" },\n",
    ")\n",
    "\n",
    "conf.widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bcd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you get a JavaScript error when running the above cell, it is likely due \n",
    "## to Anywidget needing to be installed before the workspace is launched.\n",
    "\n",
    "## Check that anywidget is installed\n",
    "## Uncomment the following line:\n",
    "# import anywidget \n",
    "\n",
    "## If it is not yet installed:\n",
    "# !pip install anywidget\n",
    "\n",
    "## Once you have checked that it is installed, close this window.\n",
    "## In the Workspace overview page, stop all jobs.\n",
    "## Then, launch the Workspace again."
   ]
  }
]
